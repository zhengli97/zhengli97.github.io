<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Zheng Li (李政) | Home</title>
  <meta name="description" content="Zheng Li">
  <meta name="author" content="Zheng Li">
  <meta property="og:title" content="Zheng Li" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://zhengli97.github.io" />
  <meta property="og:site_name" content="Zheng Li" />
  <link rel="canonical" href="https://zhengli97.github.io"/>

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– 
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css> -->
  <link rel="stylesheet" href=/libs/external/fontawesome-free-6.6.0-web/css/all.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img class="u-max-full-width" src='/assets/pics/touxiang3.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>Zheng Li (李政)</h1>
            <p>PhD Student, Nankai University</p>
            <p>zhengli97 [AT] foxmail.com</p>
            <p>Shanghai, China</p>
            <p>
              <!-- <span onclick="window.open('')" style="cursor: pointer">
                <i class="fa-brands fa-bluesky" style="padding-top: 10px"></i>
              </span>

              <span onclick="window.open('')" style="cursor: pointer">
                <i class="fa-brands fa-twitter fa-lg"></i>
              </span>

              <span onclick="window.open('')" style="cursor: pointer">
                <i class="fa-brands fa-linkedin fa-lg"></i>
              </span> -->

              <span onclick="window.open('https://www.researchgate.net/profile/Zheng-Li-132')" style="cursor: pointer">
                <i class="fa-brands fa-researchgate"></i> ResearchGate 
              </span>

              <span onclick="window.open('https://github.com/zhengli97')" style="cursor: pointer">
                <i class="fa-brands fa-github fa-lg"></i> Github 
              </span>

              <span onclick="window.open('https://scholar.google.com/citations?user=qhKtNSIAAAAJ&hl=en')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i> Google Scholar 
              </span>
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#bio>Bio</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#news>News</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=/index.html#prospective-students>Prospective Students</a></li> -->
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Publications</a></li>
          <!--<li class="navbar-item"><a class="navbar-link" href=/index.html#projects>Projects</a></li>-->
          <li class="navbar-item"><a class="navbar-link" href=/index.html#experience>Experiences</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#misc>Misc</a></li>
        </ul>
      </div>
    </nav>

    <!-- ========== BIO ========== -->
<div class="docs-section" id="bio">
  <h4>Bio</h4>
  <p>
    I am currently a forth-year Ph.D. student at Tianjin Key Laboratory of Visual Computing and Intelligent Perception (VCIP), 
    Nankai University, advised by <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ" target="_blank">Prof. Xiang Li</a> 
    and <a href="https://scholar.google.com/citations?user=6CIDtZQAAAAJ" target="_blank">Prof. Jian Yang</a>. 
    <!-- I am also a research intern at Alibaba DAMO Academy, mentored by <a href="https://scholar.google.com/citations?user=oRhJHmIAAAAJ" target="_blank">Dr. Yibing Song</a>. -->
    <!-- My research mainly focuses on vision-language models, multi-modal learning and efficient model computing. -->
    My research mainly focus on the following aspects:
    <ul>
      <li>Efficient Model Compression: [<a href="https://arxiv.org/abs/2211.16231">CTKD</a>] [<a href="https://www.sciencedirect.com/science/article/pii/S0031320324001730">DTSKD</a>] [<a href="https://arxiv.org/abs/2108.02092">OKDHP</a>] </li>
      <li>Efficient Model Finetuning & VLMs: [<a href="https://arxiv.org/abs/2511.21188">AnchorOPT</a>] [<a href="https://arxiv.org/abs/2412.09442">ATPrompt</a>] [<a href="https://arxiv.org/abs/2403.02781">PromptKD</a>] </li>
    </ul>
  </p>

  <!-- <p>
    I am also maintaining a curated list <a href="https://github.com/zhengli97/Awesome-Prompt-Learning-for-Vision-Language-Models" target="_blank">[Links]</a>
    <img src="https://img.shields.io/github/stars/zhengli97/Awesome-Prompt-Learning-for-Vision-Language-Models?style=social"/> 
    of prompt learning methods for vision-language models. Feel free to check it out~
  </p> -->
  <p>
    The code of my research work will be open-source, and I will also attach a detailed Chinese interpretation of the paper. 
    Although the interpretation may be somewhat fragmented, I will do my best to present the insights and ideas behind the paper.
  </p><p>
    The journey of scientific research is challenging, but I'm passionate about my work. 
    If you're interested in my research or encounter any problems, feel free to contact me via email (zhengli97 [at] qq.com).
  </p>
</div>


<div class="docs-section" id="news">
  <h4>News</h4>
  <ul>
    <li>Nov.2025. Our new work AnchorOPT is now avavilable on arxiv. <a href="https://arxiv.org/abs/2511.21188">[Link]</a> <a href="https://github.com/zhengli97/ATPrompt">[Code]</a> </li>
    <li>June.2025. ATPrompt is accpeted by ICCV 2025. </li>
    <li>Dec.2024. Our new work ATPrompt is now avavilable on arXiv. <a href="https://arxiv.org/abs/2412.09442">[Link]</a> <a href="https://github.com/zhengli97/ATPrompt">[Code]</a> <a href="https://zhengli97.github.io/ATPrompt/">[Project Page]</a>.</li>
    <li>Oct.2024. I was invited to give a talk on TechBeat (将门) about prompt learning methods. <a href="https://www.techbeat.net/talk-info?id=915">[Link]</a></li>
    <li>Mar.2024. We release a curated <a href="https://github.com/zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs">list</a> of prompt/adapter learning methods for VLMs. Feel free to check it out~</li>
    <li>Feb.2024. <a href="https://arxiv.org/abs/2403.02781">PromptKD</a>  is accepted by CVPR 2024. Code is public avavilable at <a href="https://github.com/zhengli97/PromptKD">GitHub</a>.</li>
    <details>
      <summary><li>&ensp;Before 2024: </li></Summary>
      <li>May.2023. <a href="https://arxiv.org/abs/2305.12954">DM-KD</a> is now avavilable on arxiv. Code is public avavilable at <a href="https://github.com/zhengli97/DM-KD">Github</a>.</li>
      <li>Jan.2023. <a href="https://arxiv.org/abs/2211.16231">CTKD</a> is accepted by AAAI 2023. Code is public avavilable at <a href="https://github.com/zhengli97/CTKD">Github</a>.</li>
     </details>
  </ul>
</div>


<!-- ========== PUBLICATIONS ========== -->
<div class="docs-section" id="publications">
  <h4>Publications</h4>

  <p>Most recent publications are on <a href="https://scholar.google.com/citations?user=qhKtNSIAAAAJ&hl=en" target="_blank">Google Scholar</a>.<br/>
  <sup>#</sup> indicates corresponding author.
  </p>

  <ul class="tab-nav">
    <li><div class="button active" data-ref="#papers-selected">Selected</div></li>
    <li><div class="button" data-ref="#papers-all">All</div></li>
  </ul>

  <div class="tab-content">
    <div class="tab-pane active" id="papers-selected">
      
      
        <div class="paper">
          <p class="title"><b>AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning.</b></p>
          <p><b>Zheng Li</b>, Yibing Song, Xin Zhang, Lei Luo, Xiang Li<sup>#</sup>, Jian Yang<sup>#</sup>.</p>
          <p><b>arxiv 2025</b></p>
          <p>AnchorOPT transforms anchors from explicit static tokens into implicitly optimized representations, further improving the performance of anchor-guided prompt learning.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://arxiv.org/abs/2511.21188" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/ATPrompt" target="_blank">Code</a>
            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Advancing Textual Prompt Learning with Anchored Attributes.</b></p>
          <p><b>Zheng Li</b>, Yibing Song, Ming-Ming Cheng, Xiang Li<sup>#</sup>, Jian Yang<sup>#</sup>.</p>
          <p><b>ICCV 2025</b></p>
          <p>ATPrompt introduces a new attribute-anchored prompt format that can be seamlessly integrated into existing textual prompt learning methods and achieve general improvements.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://arxiv.org/abs/2412.09442" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/ATPrompt" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://zhengli97.github.io/ATPrompt/" target="_blank">Project Page</a>
            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/11787739769" target="_blank">中文解读</a>
            

            

            
            <a class="button" href="https://github.com/zhengli97/ATPrompt/blob/main/docs/ATPrompt_chinese_version.pdf" target="_blank">中文版</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>PromptKD: Unsupervised Prompt Distillation for Vision-Language Models.</b></p>
          <p><b>Zheng Li</b>, Xiang Li<sup>#</sup>, Xinyi Fu, Xin Zhang, Weiqiang Wang, Shuo Chen, Jian Yang<sup>#</sup></p>
          <p><b>CVPR 2024</b></p>
          <p>PromptKD is a simple and effective <i>prompt-driven unsupervised distillation framework</i> for VLMs (e.g., CLIP), with state-of-the-art performance.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://arxiv.org/abs/2403.02781" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/PromptKD" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://zhengli97.github.io/PromptKD/" target="_blank">Project Page</a>
            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/684269963" target="_blank">中文解读</a>
            

            
              <a class="button" href="https://www.techbeat.net/talk-info?id=915" target="_blank">视频解读</a>
            

            
            <a class="button" href="https://github.com/zhengli97/PromptKD/blob/main/docs/PromptKD_chinese_version.pdf" target="_blank">中文版</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dual Teachers for Self-Knowledge Distillation.</b></p>
          <p><b>Zheng Li</b>, Xiang Li, Lingfeng Yang, Renjie Song, Jian Yang<sup>#</sup>, Zhigeng Pan.</p>
          <p><b>PR 2024</b></p>
          <p>DTSKD explores a new self-KD framework where the student network receives self-supervisions by <i>dual teachers</i> from two dramatically distinct fields.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://www.sciencedirect.com/science/article/pii/S0031320324001730" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/DTSKD" target="_blank">Code</a>
            

            

            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/690877571" target="_blank">中文解读</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Curriculum Temperature for Knowledge Distillation.</b></p>
          <p><b>Zheng Li</b>, Xiang Li<sup>#</sup>, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, Jian Yang<sup>#</sup>.</p>
          <p><b>AAAI 2023</b></p>
          <p>CTKD organizes the distillation task from easy to hard through a <i>dynamic and learnable temperature</i>.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://arxiv.org/abs/2211.16231" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/CTKD" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://zhengli97.github.io/CTKD" target="_blank">Project Page</a>
            

            
              <a class="button" href="https://zhengli97.github.io/CTKD/chinese_interpertation.html" target="_blank">中文解读</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Online Knowledge Distillation for Efficient Pose Estimation.</b></p>
          <p><b>Zheng Li</b>, Jingwen Ye, Mingli Song, Ying Huang, Zhigeng Pan<sup>#</sup>.</p>
          <p><b>ICCV 2021</b></p>
          <p>OKDHP first proposes to distill the pose structure knowledge in a <i>one-stage</i> manner.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://arxiv.org/abs/2108.02092" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/OKDHP" target="_blank">Code</a>
            

            

            

            
              <a class="button" href="https://zhengli97.github.io/OKDHP/chinese_interpertation.html" target="_blank">中文解读</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Online Knowledge Distillation via Multi-branch Diversity Enhancement.</b></p>
          <p><b>Zheng Li</b>, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan<sup>#</sup>.</p>
          <p><b>ACCV 2020</b></p>
          <p>OKDMDE is a simple and effective technique to enhance <i>model diversity</i> in online knowledge distillation.</p>
          <div class="paper-buttons">
            
              <!-- <a class="button" href="" target="_blank">Paper</a> -->
              <a class="button" href="https://arxiv.org/abs/2010.00795" target="_blank">Paper</a>
            

            

            

            

            

            

            

            

            

            

          </div>
        </div>
      
    </div>

    <!-- for all papers -->
    <div class="tab-pane" id="papers-all">
      
        <div class="paper">
          <p class="title"><b>AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning.</b></p>
          <p><b>Zheng Li</b>, Yibing Song, Xin Zhang, Lei Luo, Xiang Li<sup>#</sup>, Jian Yang<sup>#</sup>.</p>
          <p><i>arxiv 2025</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2511.21188" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/ATPrompt" target="_blank">Code</a>
            

            

            

            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Advancing Textual Prompt Learning with Anchored Attributes.</b></p>
          <p><b>Zheng Li</b>, Yibing Song, Ming-Ming Cheng, Xiang Li<sup>#</sup>, Jian Yang<sup>#</sup>.</p>
          <p><i>ICCV 2025</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2412.09442" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/ATPrompt" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://zhengli97.github.io/ATPrompt/" target="_blank">Project Page</a>
            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/11787739769" target="_blank">中文解读</a>
            
             
            

            
              <a class="button" href="https://github.com/zhengli97/ATPrompt/blob/main/docs/ATPrompt_chinese_version.pdf" target="_blank">中文版</a>
            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Cascade Prompt Learning for Vision-Language Model Adaptation.</b></p>
          <p>Ge Wu, Xin Zhang, <b>Zheng Li</b>, Zhaowei Chen, Jiajun Liang, Jian Yang, Xiang Li<sup>#</sup>.</p>
          <p><i>ECCV 2024</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2409.17805" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/megvii-research/CasPL" target="_blank">Code</a>
            

            

            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/867291664" target="_blank">中文解读</a>
            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>PromptKD: Unsupervised Prompt Distillation for Vision-Language Models.</b></p>
          <p><b>Zheng Li</b>, Xiang Li<sup>#</sup>, Xinyi Fu, Xin Zhang, Weiqiang Wang, Shuo Chen, Jian Yang<sup>#</sup></p>
          <p><i>CVPR 2024</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2403.02781" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/PromptKD" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://zhengli97.github.io/PromptKD/" target="_blank">Project Page</a>
            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/684269963" target="_blank">中文解读</a>
            
             
            
              <a class="button" href="https://www.techbeat.net/talk-info?id=915" target="_blank">视频解读</a>
            

            
              <a class="button" href="https://github.com/zhengli97/PromptKD/blob/main/docs/PromptKD_chinese_version.pdf" target="_blank">中文版</a>
            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dual Teachers for Self-Knowledge Distillation.</b></p>
          <p><b>Zheng Li</b>, Xiang Li, Lingfeng Yang, Renjie Song, Jian Yang<sup>#</sup>, Zhigeng Pan.</p>
          <p><i>PR 2024</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://www.sciencedirect.com/science/article/pii/S0031320324001730" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/DTSKD" target="_blank">Code</a>
            

            

            

            
              <a class="button" href="https://zhuanlan.zhihu.com/p/690877571" target="_blank">中文解读</a>
            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>GEIKD: Self-knowledge Distillation based on Gated Ensemble Networks and Influences-based Label Noise Removal.</b></p>
          <p>Fuchang Liu, Yu Wang, <b>Zheng Li</b>, Zhigeng Pan<sup>#</sup>.</p>
          <p><i>CVIU 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://www.sciencedirect.com/science/article/pii/S1077314223001510" target="_blank">Paper</a>
            

            

            

            

            

            

            

            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?</b></p>
          <p><b>Zheng Li</b>, Yuxuan Li, Penghai Zhao, Renjie Song, Xiang Li<sup>#</sup>, Jian Yang<sup>#</sup>.</p>
          <p><i>Arxiv Preprint 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.12954" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/DM-KD" target="_blank">Code</a>
            

            

            

            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Curriculum Temperature for Knowledge Distillation.</b></p>
          <p><b>Zheng Li</b>, Xiang Li<sup>#</sup>, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, Jian Yang<sup>#</sup>.</p>
          <p><i>AAAI 2023</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2211.16231" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/CTKD" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://zhengli97.github.io/CTKD" target="_blank">Project Page</a>
            

            
              <a class="button" href="https://zhengli97.github.io/CTKD/chinese_interpertation.html" target="_blank">中文解读</a>
            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Online Knowledge Distillation for Efficient Pose Estimation.</b></p>
          <p><b>Zheng Li</b>, Jingwen Ye, Mingli Song, Ying Huang, Zhigeng Pan<sup>#</sup>.</p>
          <p><i>ICCV 2021</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2108.02092" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/zhengli97/OKDHP" target="_blank">Code</a>
            

            

            

            
              <a class="button" href="https://zhengli97.github.io/OKDHP/chinese_interpertation.html" target="_blank">中文解读</a>
            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Online Knowledge Distillation via Multi-branch Diversity Enhancement.</b></p>
          <p><b>Zheng Li</b>, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan<sup>#</sup>.</p>
          <p><i>ACCV 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2010.00795" target="_blank">Paper</a>
            

            

            

            

            

            

            

            
             
            

            
             
          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dream-experiment: a MR user interface with natural multi-channel interaction for virtual experiments.</b></p>
          <p>Tianren Luo, Mingmin Zhang<sup>#</sup>, Zhigeng Pan<sup>#</sup>, <b>Zheng Li</b>, Ning Cai, Jinda Miao, Youbin Chen, Mingxi Xu.</p>
          <p><i>TVCG 2020</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/9199566" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=HfIQz4min1E" target="_blank">Video</a>
            

            

            

            

            
             
            

            
             
          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="docs-section" id="experience">
  <h4>Experiences</h4>
  <h5>Education</h5>
  <ul>
    <li>Sep.2022 - Present. Ph.D., Computer Science and Technology, Nankai University. </li>
    <li>Sep.2019 - June.2022. M.Eng., Computer Applied Technology, Hangzhou Normal University.</li>
    <li>Sep.2015 - June.2019. B.Eng., Telecommunication Engineering, North China University of Science and Technology.</li>
  </ul>

  <h5>Internship</h5>
  <ul>
    <li>Aug.2025 - Present. Wechat Pattern Recognition Center, WXG, Tencent, Shanghai. Intern. </li>
    <li>June.2024 - July. 2025. <a href="https://damo.alibaba.com/">DAMO Academy</a>, Alibaba Group, Hangzhou. Research Intern. Mentored by <a href="https://scholar.google.com/citations?user=oRhJHmIAAAAJ">Dr. Yibing Song</a>.</li>
    <li>Sep.2023 - June.2024. Security and Risk Management Department, <a href="https://www.antgroup.com/">Ant Group,</a> Hangzhou. Research Intern. Mentored by <a href="https://scholar.google.com/citations?user=t1gqx_cAAAAJ">Xinyi Fu</a> and <a href="https://scholar.google.com/citations?user=rbaNwV8AAAAJ">Dr. Xing Fu</a>.</li>
    <li>Oct.2021 - Aug.2023. <a href="https://megvii.com/">Megvii Research</a>, Nanjing. Research Intern. Mentored by <a href="https://scholar.google.com/citations?user=-EgH8oIAAAAJ">Renjie Song</a>.</li>
  </ul>

  <h5>Competition</h5>
  <h6>Kaggle Competition Master. <a href="https://www.kaggle.com/mdlszhengli">[Profile]</a></h6>
  <ul>
    <li>Nov.2019. "Understanding Clouds from Satellite Images." Rank: 7th/1538 (Top 1%). Gold Medal. <a href="https://www.kaggle.com/c/understanding_cloud_organization">[Link]</a> <a href="https://www.kaggle.com/c/understanding_cloud_organization/discussion/117974">[Solution]</a></li>
    <li>Apr.2018. "2018 Data Science Bowl." Rank: 8th/3634 (Top 1%). Solo Gold Medal. <a href="https://www.kaggle.com/c/data-science-bowl-2018">[Link]</a> <a href="https://www.kaggle.com/c/data-science-bowl-2018/discussion/54838#316229">[Solution]</a></li>
  </ul>
</div>


<div class="docs-section" id="misc">
  <h4>Misc</h4>
  <h5>Personal Hobbies</h5>
  <ul>
    <li>Badminton.</li>
    <li>Hiking & Trail Running: <a href="https://itra.run/RunnerSpace/li.zheng/5425908">[iTRA]</a>: 
      <ul>
        <li><a href="https://itra.run/Races/RaceDetails/97291">TNF100 Ultra Trail Challenge Moganshan - 30km Group.</a> Finish: 5h 28min (32km/1680m+).</li>
        <li><a href="https://itra.run/Races/RaceDetails/102061">Ultra-Trail Xiamen by UTMB - 20KM Group.</a> Finish: 4h 24min (28KM/990m+).</li>
      </ul>
    <li>Photography: I am a contracted photographer for <a href="https://500px.com.cn/">500px Gallery</a>. Here are some <a href=https://500px.com.cn/zhengliphoto"">photos</a> I took while traveling and mountaineering. </li>
  </li>
  </ul>
</div>

<!-- ========== PROJECTS ========== 
<div class="docs-section" id="projects">
  <h4>Projects</h4>

  <ul class="tab-nav">
    <li><div class="button active" data-ref="#projects-selected">Selected</div></li>
    <li><div class="button" data-ref="#projects-all">All</div></li>
  </ul>

  <div class="tab-content">
    <div class="tab-pane active" id="projects-selected">
      
      
    </div>

    <div class="tab-pane" id="projects-all">
      
    </div>
  </div>

</div>
-->

<!-- ========== RESUME ========== -->
<!-- <div class="docs-section" id="resume">
  <h4>Vitæ</h4>

  <p>Full Resume in <a href=/assets/cv/cv_web.pdf target="_blank">PDF</a>.</p> -->

  <!-- The Timeline -->
  <!-- <ul class="timeline">
    
  </ul>
</div> -->

<!-- <div class="docs-section" id="template">
  <h4>Website Design</h4>
  Since I made this website, many people have found this Jekyll template useful [
  
  ].
  <br/>-->
  <!-- You can find all the code needed to build your website in this <a href="https://github.com/msaveski/www_personal" target="_blank">GitHub repo</a>.
  Feel free to use it. <br/>

  If you end up using it, please link to here and drop me an email.
  I'm always happy to see people using it.
</div>   -->


    <div class="footer">
      <div class="row">
        <div class="four columns">
          Zheng Li (李政)
        </div>
        <div class="four columns">
          Last Update: 2025/11
        </div>
        <div class="four columns">
          <!-- <span onclick="window.open('')" style="cursor: pointer">
            <i class="fa-brands fa-bluesky fa-sm" style="padding-top: 10px"></i>
          </span>

          <span onclick="window.open('')" style="cursor: pointer">
            <i class="fa-brands fa-twitter"></i>
          </span>

          <span onclick="window.open('')" style="cursor: pointer">
            <i class="fa-brands fa-linkedin"></i> -->
          <!-- </span> -->

          <!-- <span onclick="window.open('https://github.com/zhengli97')" style="cursor: pointer">
            <i class="fa-brands fa-github"></i>
          </span>

          <span onclick="window.open('https://scholar.google.com/citations?user=qhKtNSIAAAAJ&hl=en')" style="cursor: pointer">
            <i class="ai ai-google-scholar" aria-hidden="true"></i>
          </span> -->
          <div class="ten columns">
            Per Aspera Ad Astra.
          </div>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>

  <!-- do not remove -->
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
